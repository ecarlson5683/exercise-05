---
title: "exercise-05"
author: "Erika Carlson"
date: 2024-03-03
format: html
editor_options: 
  chunk_output_type: console
---

# Exercise 05 {.unnumbered}

# Generate Sampling Distributions and CIs {.unnumbered}

## Challenge 1 {.unnumbered}

#### Step 1 {.unnumbered}
- Loaded the "IMDB-movies.csv" dataset as a "tibble" named **d**

```{r}
#| warning: false
library(tidyverse)
d <- read_csv("https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/IMDB-movies.csv",
              col_names = TRUE)
```

#### Step 2 {.unnumbered}
- Filtered the dataset to include just movies from 1920 to 1979 and movies that are between 1 and 3 hours long (**runtimeMinutes** >= 60 and **runtimeMinutes** <= 180), and added a new column that codes the **startYear** into a new variable, **decade** ("20s", "30s", ..."70s").

```{r}
d <- d %>% 
  filter(startYear >= 1920 & startYear <= 1979 & runtimeMinutes >= 60 & runtimeMinutes <=180) %>%
  mutate(decade = case_when(startYear < 1930 ~ "20s",
                            startYear < 1940 ~ "30s",
                            startYear < 1950 ~ "40s",
                            startYear < 1960 ~ "50s",
                            startYear < 1970 ~ "60s",
                            startYear < 1980 ~ "70s"))

(length(d$primaryTitle)) # number of movies in filtered data set
```

#### Step 3 {.unnumbered}
- Used {ggplot2} to plot histograms of the distribution of **runtimeMinutes** for each decade.

```{r}
p <- ggplot(d, aes(runtimeMinutes)) +
  geom_histogram() +
  xlab("Run Time (min)") +
  facet_wrap(vars(decade))
p
```

#### Step 4 {.unnumbered}

- Calculated the population mean and population standard deviation in **runtimeMinutes** for each decade and saved the results in a new dataframe called **results**.

```{r}
results <- d %>% 
  group_by(decade) %>% 
  summarize(mu = mean(runtimeMinutes),
          sigma = sqrt(sum((runtimeMinutes - mean(runtimeMinutes))^2)/length(runtimeMinutes)))
```

#### Step 5 {.unnumbered}

- Drew a single sample of 100 movies, without replacement, from each decade and calculated the single sample mean and single sample standard deviation in **runtimeMinutes** for each decades. 

```{r}
s <- d %>% group_by(decade) %>% slice_sample(n = 100, replace = FALSE)

s_results <- s %>% group_by(decade) %>% 
  summarize(mean = mean(runtimeMinutes),
            sd = sd(runtimeMinutes)) # sd() divides by n - 1
```

#### Step 6 {.unnumbered}

- Calculated for each decade the standard error around my estimate of the population mean **runtimeMinutes** based on the standard deviation and sample size (n=100 movies) of my single sample.

```{r}
s_results <- s %>% group_by(decade) %>% 
  summarize(mean = mean(runtimeMinutes),
            sd = sd(runtimeMinutes),
            se = sd / sqrt(length(runtimeMinutes))) # added sample SE to sample results
```


# DONE TO HERE


#### Step 7 {.unnumbered}

- Compared these estimates to the actual population mean **runtimeMinutes** for each decade and to the calculated SE in the population mean for samples of size 100 based on the population standard deviation for each decade.

**ToDo: Specific way to compare? With statistics?**

```{r}
results <- d %>% 
  group_by(decade) %>% 
  summarize(mu = mean(runtimeMinutes),
            sigma = sqrt(sum((runtimeMinutes - mean(runtimeMinutes))^2)/length(runtimeMinutes)),
            se_pop = sigma / sqrt(100)) # added population-based standard error to results

results_pop_samp <- full_join(s_results, results) %>% select(decade, mu, mean, sigma, sd, se_pop, se)
```

#### Step 8 {.unnumbered}

- Generated a *sampling distribution* of mean **runtimeMinutes** for each decade by [a] drawing 1000 random samples of 100 movies from each decade, without replacement, and, for each sample, [b] calculating the mean **runtimeMinutes** and the standard deviation in **runtimeMinutes** for each decade. 
- Could use a standard `for( ){ }` loop, the `do(reps) *` formulation from {mosaic}, the `rerun()` function from {purrr}, or the `rep_sample_n()` workflow from {infer} to generate these sampling distributions (see [**Module 16**](#module-16)).

**ToDo: Stuck on grouping a list by some factor (in this case to get mean/sd estimates of each sample by decade)**

##### Attempt 1 (failed strategy)
Failed, unable to figure out how to extract summary data from lists
```{r}
#| eval: false

sample_size <- 100 # size of each sample
num_samples <- 1000 # number of samples

# create a dummy variable to hold the sample distributions
samp_dist <- list()

# set the seed so the sample is the same with every run
set.seed(1) 

# loop 1000 iterations, calculate summary stats for each decade as it goes and just save those
for (i in 1:num_samples) {
  samp_dist_sum[[i] <- d %>% group_by(decade) %>% 
                  slice_sample(n = sample_size, replace = FALSE)
}

sample_mean <- vector(length = num_samples)  # create a dummy variable to hold the mean of each sample
# here the dummy variable can be a vector and we can preallocate its length
for (i in 1:num_samples) {
    sample_mean[[i]] <- mean(samp_dist[[i]]) # how to group by decade?
}

sample_sd <- vector(length = num_samples)  # create a dummy variable to hold the SD of each sample
for (i in 1:num_samples) {
    sample_sd[[i]] <- sd(samp_dist[[i]]) # how to group by decade?
}
      
```


##### Attempt 2 (failed strategy)
Can't figure out syntax for outputting summary stats per decade per sample into tibble
```{r}
#| eval: false

sample_size <- 100 # size of each sample
num_samples <- 1000 # number of samples

# set the seed so the sample is the same with every run
set.seed(1) 

# create a dummy variable to hold the summary statistics for sample distribution
# need per decade?
samp_dist_sum <- tibble(samp_mean_20 = integer(length = num_samples), 
                        samp_sd_20 = integer(length = num_samples), 
                        samp_se_20 = integer(length = num_samples), 
                        samp_mean_30 = integer(length = num_samples), 
                        samp_sd_30 = integer(length = num_samples), 
                        samp_se_30 = integer(length = num_samples),
                        samp_mean_40 = integer(length = num_samples), 
                        samp_sd_40 = integer(length = num_samples), 
                        samp_se_40 = integer(length = num_samples), 
                        samp_mean_50 = integer(length = num_samples), 
                        samp_sd_50 = integer(length = num_samples), 
                        samp_se_50 = integer(length = num_samples),
                        samp_mean_60 = integer(length = num_samples), 
                        samp_sd_60 = integer(length = num_samples), 
                        samp_se_60 = integer(length = num_samples),
                        .rows = num_samples, 
                        .name_repair = unique)  

# loop 1000 iterations, calculate summary stats for each decade as it goes and just save those
for (i in 1:num_samples) {
  # save output of a single string?
    samp_dist_sum[i, ] <- d %>% group_by(decade) %>% 
      slice_sample(n = sample_size, replace = FALSE) %>% 
      summarize(samp_mean = mean(runtimeMinutes),
                samp_sd = sd(runtimeMinutes),
                samp_se = sd(runtimeMinutes)/sqrt(sample_size))
}
```

##### Attempt 3
Function version
```{r}
# set the seed so the sample is the same with every run
set.seed(1)  

# Function to create sampling distribution and calculate summary statistics
create_sampling_distribution <- function(x, sample_size = 100, num_samples = 1000) {
  # create dummy data frame to store summary statistics
  samp_dist <- data.frame(samp_mean = numeric(num_samples),
                          samp_sd = numeric(num_samples),
                          samp_se = numeric(num_samples),
                          decade = character(num_samples))
  
  # loop 1000 iterations to create sampling distribution
  for (i in 1:num_samples) {
    # Randomly sample from the data
    sample_data <- slice_sample(x, n = sample_size, replace = FALSE)
    
    # Calculate summary statistics for the sample
    samp_mean <- mean(sample_data$runtimeMinutes)
    samp_sd <- sd(sample_data$runtimeMinutes)
    samp_se <- samp_sd / sqrt(sample_size)
    
    # And store the decade of each sample
    decade <- sample_data$decade
    
    # Store summary statistics and corresponding decade in the data frame
    samp_dist[i, ] <- c(samp_mean, samp_sd, samp_se, decade)
  }
  
  return(samp_dist)
}

# Apply the function to each decade using `split()` and combine the results
samp_dist <- map_dfr(split(d, d$decade), create_sampling_distribution)

# Check the result
str(samp_dist)

# Change numeric variables back to numeric
samp_dist <- samp_dist %>% mutate_at(c('samp_mean', 'samp_sd', 'samp_se'), as.numeric)
```

#### Step 9 {.unnumbered}

- Then, calculate the **mean** and the **standard deviation** of the sampling distribution of sample means for each decade (the former should be a very good estimate of the population mean, while the latter is another estimate of the standard error in our estimate of the population mean for a particular sample size) and plot a histogram of the sampling distribution for each decade. What shape does it have?

```{r}
sample_size <- 100 # size of each sample
num_samples <- 1000 # number of samples

samp_dist_sm <- samp_dist %>% 
  group_by(decade) %>% 
  summarize(mean = mean(samp_mean), # this is the mean of our set of sample means
            sd = sd(samp_mean)) # this is the sd of our set of sample means

(p <- ggplot(data = samp_dist, 
             aes(x = samp_mean)) + 
    geom_histogram() + 
    facet_wrap(~decade) +
    labs(title = paste0("Sampling distribution of sample means (movie run time) by decade"), 
         subtitle = paste0("Means of ", num_samples, " samples of size ", sample_size)) + 
    xlab("Sample mean (min)") + ylab("Frequency")
)
```

#### Step 10 {.unnumbered}

- Finally, compare the standard error in **runtimeMinutes** for samples of size 100 from each decade [1] as estimated from your **first** sample of 100 movies, [2] as calculated from the known *population* standard deviations for each decade, and [3] as estimated from the sampling distribution of sample means for each decade.

```{r}
# add standard error to sampling distribution of sample means summary table
samp_dist_sm <- samp_dist %>% 
  group_by(decade) %>% 
  summarize(mean = mean(samp_mean), # this is the mean of our set of sample means
            sd = sd(samp_mean), # this is the sd of our set of sample means
            se = sd/sqrt(num_samples)) # this is the se of our set of sample means

(results_pop_samp$se)
(results_pop_samp$se_pop)
(samp_dist_sm$se)
```


## Challenge 2 {.unnumbered}

#### Step 1 {.unnumbered}

- Loaded the "zombies.csv" dataset as a "tibble" named **z**. This dataset includes the first and last name and gender of the **entire** population of 1000 people who have survived the zombie apocalypse and are now eking out an existence somewhere on the Gulf Coast, along with several other variables (height, weight, age, number of years of education, number of zombies they have killed, and college major).

```{r}
z <- read_csv("https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/zombies.csv", 
              col_names = TRUE)
head(z)
```

#### Step 2 {.unnumbered}

- Calculate the *population* mean and standard deviation for each quantitative random variable in the dataset (height, weight, age, number of zombies killed, and years of education).

> **NOTE:** You will **not** want to use the built in `var()` and `sd()` commands as those are for *samples*.

```{r}
pop <- z %>% select(height, weight, age, zombies_killed, years_of_education)

# Method 1: loop to calculate mean, returns vector without column names
pop_mean <- vector("double", ncol(pop))
for (i in seq_along(pop)) {
    pop_mean[[i]] <- mean(pop[[i]], na.rm = TRUE)
}

# Method 2: Using `sapply()` to calculate mean, which keeps column names
pop_mean <- sapply(pop, FUN = mean, na.rm = TRUE)

# Method 3: Using `map_dfr()` to return a data frame
pop_mean <- map_dfr(pop, .f = mean, na.rm = TRUE)


# Method 1: loop to calculate sd, returns vector without column names
pop_sd <- vector("double", ncol(pop))
for (i in seq_along(pop)) {
    pop_sd[[i]] <- sqrt(sum((pop[[i]] - mean(pop[[i]]))^2)/length(pop[[i]]))
}

# Method 2: Using `sapply()` to calculate sd, which keeps column names
# first, write a function to calculate population standard deviation
# should include na.rm = TRUE ?
sd_pop <- function(x) {
  sqrt(sum((x - mean(x))^2)/(length(x)))
}

pop_sd <- sapply(pop, FUN = sd_pop) # then apply function to columns of data frame

# Method 3: Using `map_dfr()` to return a data frame
pop_sd <- map_dfr(pop, .f = sd_pop)

```

#### Step 3 {.unnumbered}

- Use {ggplot} and make boxplots of each of these variables by gender.

```{r}
# first, we use `tidyr::pivot_longer()` to convert our data from wide to long
# format this is so we can use `facet.grid()`

z_long <- pivot_longer(z, c("height", "weight", "age", "zombies_killed", "years_of_education"), 
                       names_to = "Variable",
                       values_to = "Value")

p <- ggplot(data = z_long, aes(x = factor(0), y = Value)) + 
  geom_boxplot(aes(color = gender), na.rm = TRUE) + 
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    facet_grid(. ~ Variable)
p

# y-axis range is pretty different for weight, better to plot separately?
p <- ggplot(data = z, aes(color = factor(gender))

p1 <- p +
  geom_boxplot(z, mapping = aes(y = height), na.rm = TRUE) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Height")

p2 <- p +
  geom_boxplot(z, mapping = aes(y = weight), na.rm = TRUE) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Weight")

p3 <- p +
  geom_boxplot(z, mapping = aes(y = age), na.rm = TRUE) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Age")

p4 <- p +
  geom_boxplot(z, mapping = aes(y = zombies_killed), na.rm = TRUE) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Zombies killed")

p5 <- p +
  geom_boxplot(z, mapping = aes(y = years_of_education), na.rm = TRUE) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Years of education")

library(ggpubr)
ggarrange(p1, p2, p3, p4, p5 + 
            rremove("x.text"), 
          labels = NULL,
          ncol = 3, nrow = 2)

```

#### Step 4 {.unnumbered}

- Use {ggplot} and make scatterplots of height and weight in relation to age (i.e., use age as the $x$ variable), using different colored points for males versus females. Do these variables seem to be related? In what way?

```{r}
# build a plot object, color points by gender, and set age as x variable 
# set height as y variable
# then add scatterplots
# then add linear model regression lines to visualize relationship between variables
p1 <- ggplot(data = z, aes(y = height, x = age, color = factor(gender))) + 
  ylab("Height") + xlab("Age") +
  geom_point(na.rm = TRUE) + 
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_smooth(method = "lm", fullrange = FALSE, na.rm = TRUE)

# repeat for y variable weight
p2 <- ggplot(data = z, aes(y = weight, x = age, color = factor(gender))) + 
  ylab("Weight") + xlab("Age") +
  geom_point(na.rm = TRUE) + 
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_smooth(method = "lm", fullrange = FALSE, na.rm = TRUE)

library(ggpubr)
ggarrange(p1, p2 + rremove("x.text"), 
          labels = NULL,
          ncol = 2, nrow = 1)

# Both height and weight seem positively correlated with age for males and females
```

#### Step 5 {.unnumbered}

- Using histograms and Q-Q plots, check whether each of the quantitative variables seem to be drawn from a normal distribution. Which seem to be and which do not?

```{r}
# Should attempt alternative that does not require as much copy and pasting

par(mfrow = c(2, 3))  # set up multiple panels for histograms

hist(z$height, freq = FALSE, col = "white", main = "Density Plot with Mean",
    xlab = "Height", ylab = "density", ylim = c(0, 0.1))
abline(v = mean(z$height, na.rm = TRUE), col = "blue")
lines(density(z$height, na.rm = TRUE), col = "green")

hist(z$weight, freq = FALSE, col = "white", main = "Density Plot with Mean",
    xlab = "Weight", ylab = "density", ylim = c(0, 0.025))
abline(v = mean(z$weight, na.rm = TRUE), col = "blue")
lines(density(z$weight, na.rm = TRUE), col = "green")

hist(z$age, freq = FALSE, col = "white", main = "Density Plot with Mean",
    xlab = "Age", ylab = "density", ylim = c(0, 0.2))
abline(v = mean(z$age, na.rm = TRUE), col = "blue")
lines(density(z$age, na.rm = TRUE), col = "green")

hist(z$zombies_killed, freq = FALSE, col = "white", main = "Density Plot with Mean",
    xlab = "Zombies Killed", ylab = "density", ylim = c(0, 0.3))
abline(v = mean(z$zombies_killed, na.rm = TRUE), col = "blue")
lines(density(z$zombies_killed, na.rm = TRUE), col = "green")

hist(z$years_of_education, freq = FALSE, col = "white", main = "Density Plot with Mean",
    xlab = "Years of Education", ylab = "density", ylim = c(0, 0.3))
abline(v = mean(z$years_of_education, na.rm = TRUE), col = "blue")
lines(density(z$years_of_education, na.rm = TRUE), col = "green")
```

```{r}
# Should attempt alternative that does not require as much copy and pasting

par(mfrow = c(2, 3))  # set up multiple panels for Q-Q plots

qqnorm(z$height, main = "QQ Plot - Height")
qqline(z$height, col = "gray")

qqnorm(z$weight, main = "QQ Plot - Weight")
qqline(z$weight, col = "gray")

qqnorm(z$age, main = "QQ Plot - Age")
qqline(z$age, col = "gray")

qqnorm(z$zombies_killed, main = "QQ Plot - Zombies Killed")
qqline(z$zombies_killed, col = "gray")

qqnorm(z$years_of_education, main = "QQ Plot - Years of Education")
qqline(z$years_of_education, col = "gray")
```

```{r}
# standardize the scores
library(mosaic)

z1 <- (z$height - mean(z$height))/sd(z$height)
z1_plot <- histogram(z1, center = 0, 
                     main = paste0("Mean = ", 
                                   round(mean(z1), 3), "\nSD = ",
                                   round(sd(z1), 3)))
z1_plot


z2 <- (z$weight - mean(z$weight))/sd(z$weight)
z2_plot <- histogram(z2, center = 0, 
                     main = paste0("Mean = ", 
                                   round(mean(z2), 3), "\nSD = ",
                                   round(sd(z2), 3)))

z3 <- (z$age - mean(z$age))/sd(z$age)
z3_plot <- histogram(z3, center = 0, 
                     main = paste0("Mean = ", 
                                   round(mean(z3), 3), "\nSD = ",
                                   round(sd(z3), 3)))

z4 <- (z$zombies_killed - mean(z$zombies_killed))/sd(z$zombies_killed)
z4_plot <- histogram(z4, center = 0, 
                     main = paste0("Mean = ", 
                                   round(mean(z4), 3), "\nSD = ",
                                   round(sd(z4), 3)))

z5 <- (z$years_of_education - mean(z$years_of_education))/sd(z$years_of_education)
z5_plot <- histogram(z5, center = 0, 
                     main = paste0("Mean = ", 
                                   round(mean(z5), 3), "\nSD = ",
                                   round(sd(z5), 3)))

plot_grid(z1_plot, z2_plot, z3_plot, z4_plot, z5_plot)
```

*Answer: Height, weight, and age seem normally distributed, but zombies killed and years of education are left skewed with discrete distribution.*

> **HINT:** Not all are drawn from a normal distribution! For those that are not, can you determine what common distribution they are drawn from?

#### Step 6 {.unnumbered}

- Now use the `sample_n()` or `slice_sample()` function from {dplyr} to sample ONE subset of 50 zombie apocalypse survivors (without replacement) from this population and calculate the mean and sample standard deviation for each variable. Also estimate the standard error for each variable based on this one sample and use that to construct a theoretical 95% confidence interval for each mean. You can use either the standard normal *or* a Student's t distribution to derive the critical values needed to calculate the lower and upper limits of the CI.

```{r}
samp <- z %>% slice_sample(n = 50, replace = FALSE)

samp_mean <- samp %>% 
  select(height, weight, age, zombies_killed, years_of_education) %>% 
  map_dfr(., .f = mean, na.rm = TRUE) %>% 
  mutate(stat = "samp_mean") # add a column with type of stat for binding
  

samp_sd <- samp %>% 
  select(height, weight, age, zombies_killed, years_of_education) %>% 
  map_dfr(., .f = sd, na.rm = TRUE) %>% 
  mutate(stat = "samp_sd")

samp_sum <- rbind(samp_mean, samp_sd)
```

#### Step 7 {.unnumbered}

- Then draw another 199 random samples of 50 zombie apocalypse survivors out of the population and calculate the mean for each of the these samples. Together with the first sample you drew out, you now have a set of 200 means for each variable (each of which is based on 50 observations), which constitutes a sampling distribution for each variable. What are the means and standard deviations of the **sampling distribution** for each variable? How do the standard deviations of the sampling distribution for each variable compare to the standard errors estimated from your first sample of size 50?

#### Step 8 {.unnumbered}

- Plot the sampling distributions for each variable mean. What do they look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?

#### Step 9 {.unnumbered}
- Construct a 95% confidence interval for each mean **directly from the sampling distribution** of sample means using the central 95% that distribution (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through that distribution).

> **HINT**: You will want to use the `quantile()` function for this!

How do the various 95% CIs you estimated compare to one another (i.e., the CI based on one sample and the corresponding sample standard deviation versus the CI based on simulation where you created a sampling distribution across 200 samples)?

> **NOTE:** Remember, too, that the standard deviation of the sampling distribution is the standard error. You *could* use this value to derive yet another estimate for the 95% CI as the shape of the sampling distribution should be normal.

#### Step 10 {.unnumbered}
- Finally, use bootstrapping to generate a 95% confidence interval for each variable mean **by resampling 1000 samples, with replacement, from your original sample** (i.e., by setting the lower and upper CI bounds to 2.5% and 97.5% of the way through the sampling distribution generated by bootstrapping).
